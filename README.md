# TVM
Optimizing YOLOv8 with Apache TVM for Efficient CPU Inference This project demonstrates how to export, quantize, compile, and benchmark YOLOv8n using the Apache TVM AI compiler — enabling faster, lighter, and hardware-agnostic inference.

# 🧠 YOLOv8 Optimization with Apache TVM

This repository demonstrates how to:
- ✅ Export YOLOv8 to ONNX
- ⚙️ Compile with Apache TVM for CPU
- 📉 Apply INT8 quantization
- 🚀 Benchmark ONNXRuntime vs TVM

---

## 📦 Setup

```bash
git clone https://github.com/yourname/yolo-tvm-optimization.git
cd yolo-tvm-optimization
pip install -r requirements.txt
