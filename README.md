# TVM
Optimizing YOLOv8 with Apache TVM for Efficient CPU Inference This project demonstrates how to export, quantize, compile, and benchmark YOLOv8n using the Apache TVM AI compiler â€” enabling faster, lighter, and hardware-agnostic inference.

# ğŸ§  YOLOv8 Optimization with Apache TVM

This repository demonstrates how to:
- âœ… Export YOLOv8 to ONNX
- âš™ï¸ Compile with Apache TVM for CPU
- ğŸ“‰ Apply INT8 quantization
- ğŸš€ Benchmark ONNXRuntime vs TVM

---

## ğŸ“¦ Setup

```bash
git clone https://github.com/yourname/yolo-tvm-optimization.git
cd yolo-tvm-optimization
pip install -r requirements.txt
